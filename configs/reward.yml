model: EleutherAI/gpt-neo-125M

dataset:
  loader: reward
  name: databricks/databricks-dolly-15k
  subset_size: 5000
  max_seq_length: 512

training:
  epochs: 3
  batch_size: 4
  learning_rate: 5e-5
  logging_steps: 50

output:
  model_dir: models/reward_model
